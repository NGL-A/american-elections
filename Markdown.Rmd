---
title: "Elections, COVID and Demographic Data"
author: "A. Bissacco, F. Sartori, M. Sboarina"
date: "21/06/2022"
output:
  #word_document: default
  html_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(corrplot)
library(ggplot2)
library(ggrepel)
library(glmnet)
library(leaps)
library(mapproj)
library(maps)
library(pROC)
library(tidyverse)
library(usmap)
```

## Elections, COVID and Demographic Data

Every 4 year one of the most relevant political events in the world takes place: the election of the US President. After months, even years, of preparations (campaigns, public speeches, sponsors...) the eyes of the entire world are looking to the results of those elections, to the one who will be leading the "free world" in the near future.

Our project wants to focus on what might affect citizens vote: if the geographical area is relevant, if the type of work can change the preference, if one ethnicity is more likely to vote a Party over the other and if there is any strong correlation between the variables.  
We found our dataset on [Kaggle](https://www.kaggle.com/datasets/etsc9287/2020-general-election-polls). It contains 2017 demographic and economic situation of each American county, the numbers of 2016 and 2020 US Elections and COVID report until 1st November 2020 (few days before the elections).

#Come funzionano le elezioni

We start our analysis by removing NA values. We delete 1821 rows that are mostly counties which don't present all of the features. Counties that changed name have been classified both with the original name and the new name, but the latter contains only COVID and 2020 election data. This lead us to 3046 counties which is close to their real number of 3142^[Alaska only had the data for 2020 elections so all its boroughs had been removed.].

```{r, results='hide'}
data_uncleaned <- read.csv("county_statistics.csv", sep = ",", header = TRUE)
data <- na.omit(data_uncleaned)

```
&ensp;&ensp;  


The dataset is not consistent in the percentage notation so we turn percentages (coming from Census 2017) to relative numbers in [0,1].  

```{r}
data[,c(21:26,32:45,47:51)] <- (data[,c(21:26,32:45,47:51)]/100)
```
&ensp;&ensp;  


Our dataset contains the following 51 variables:

&ensp;&ensp;1. County: county name  
&ensp;&ensp;2. State: Two-Letter State Abbreviation  
&ensp;&ensp;3. percentage16_Donald_Trump: percentage of votes Donald Trump got in 2016  
&ensp;&ensp;4. percentage16_Hillary_Clinton: percentage of votes Hilary Clinton got in 2016  
&ensp;&ensp;5. total_votes16: total number of votes in 2016  
&ensp;&ensp;6. percentage20_Donald_Trump: percentage of votes Donald Trump got in 2020  
&ensp;&ensp;7. percentage20_Joe_Biden: percentage of votes Joe Biden got in 2020  
&ensp;&ensp;8. total_votes20: total number of votes in 2020  
&ensp;&ensp;9. lat: latitude coordinates  
&ensp;&ensp;10. long: longitude coordinates  
&ensp;&ensp;11. cases: COVID-19 cases  
&ensp;&ensp;12. deaths: COVID-19 deaths  
&ensp;&ensp;13. TotalPop: Total population  
&ensp;&ensp;14. Hispanic: percent of population that is Hispanic/Latino  
&ensp;&ensp;15. White: percent of population that is white  
&ensp;&ensp;16. Black: percent of population that is black  
&ensp;&ensp;17. Native: percent of population that is Native American or Native Alaskan  
&ensp;&ensp;18. Asian: percent of population that is Asian  
&ensp;&ensp;19. Pacific: percent of population that is Native Hawaiian or Pacific Islander  
&ensp;&ensp;20. Income: median household income  
&ensp;&ensp;21. IncomePerCap: income per capita  
&ensp;&ensp;22. Poverty: percent of population under poverty level  
&ensp;&ensp;23. Professional: percent of population employed in management, business, science and arts  
&ensp;&ensp;24. Service: percent of population employed in service jobs  
&ensp;&ensp;25. Office: percent of population employed in sales and office jobs  
&ensp;&ensp;26. Construction: percent of population employed in natural resources, construction and maintenance  
&ensp;&ensp;27. Production: percent of population employed in production, transportation and material movement  
&ensp;&ensp;28. Drive: percent commuting alone in a car, van, or truck  
&ensp;&ensp;29. Carpool: percent carpooling in a car, van, or truck  
&ensp;&ensp;30. Transit: percent commuting on public transportation  
&ensp;&ensp;31. Walk: percent walking to work OtherTransp percent commuting via other means  
&ensp;&ensp;32. WorkAtHome: percent working at home  
&ensp;&ensp;33. MeanCommute: Mean commute time (minutes)
&ensp;&ensp;34. PublicWork: percent of population employed in public jobs  
&ensp;&ensp;35. SelfEmployed: percent of population self-employed  
&ensp;&ensp;36. FamilyWork: percent of population in unpaid family work  
&ensp;&ensp;37. Unemployment: unemployment rate (percent)  



We want to see between which variables exist a deep correlation through a correlation matrix^[We changed columns "percentageXX_Candidate" into "pCandidateXX" and "votesXX_Candidate" into "CandidateXX" to obtain a clearer plot.].

```{r, results='hide', warning=FALSE, message= FALSE}
data.corr <- data
names(data.corr)[names(data.corr) == "percentage16_Donald_Trump"] <- "pTrump16"
names(data.corr)[names(data.corr) == "percentage16_Hillary_Clinton"] <- "pClinton16"
names(data.corr)[names(data.corr) == "percentage20_Donald_Trump"] <- "pTrump20"
names(data.corr)[names(data.corr) == "percentage20_Joe_Biden"] <- "pBiden20"
names(data.corr)[names(data.corr) == "votes16_Donald_Trump"] <- "Trump16"
names(data.corr)[names(data.corr) == "votes16_Hillary_Clinton"] <- "Clinton16"
names(data.corr)[names(data.corr) == "votes20_Donald_Trump"] <- "Trump20"
names(data.corr)[names(data.corr) == "votes20_Joe_Biden"] <- "Biden20"
cor <- cor(data.corr[4:51])
corrplot(cor, type = 'upper', tl.cex = 0.5)

```

We can see that "men, women blablabla" are deeply correlated because they are absolute frequencies values and they basically are linear transformation of the number of citizens (TotalPop). For example men and women are around 50-50 of the population.
We discovered that the gender doesn't affect any of the variables and that "ChildPoverty" behave like "Poverty", so we kept only the latter; while we chose "TotalPop" to represent "Men" and "Women", but also "Emplyed" and "VotingAgeCitizen". "IncomeErr" and "IncomePerCapErr" were grouped respectively with their counterpart "Income" and "IncomePerCap". "votesXX_Candidate" had been considered together with "percentageXX_Candidate".  
So we decided to maintain these:




To obtain a clearer view of the election results we add two new columns with each county winner Party in 2016 and 2020.

```{r, results='hide'}
data$Party16 = factor(if_else(data$percentage16_Hillary_Clinton >= data$percentage16_Donald_Trump, "Democrat", "Republican"))
data$Party20 = factor(if_else(data$percentage20_Joe_Biden >= data$percentage20_Donald_Trump, "Democrat", "Republican"))
```
&ensp;&ensp;  

Later on we will analyze and plot the situation in some counties in the USA so we need to fix Louisiana's ones. The state is in fact, the only one in the country which divides its territory in parishes and our dataset is missing the suffix "Parish" in the name of each subdivision.

```{r, results='hide'}
data$county[data$state == "LA"] = paste(data$county[data$state == "LA"], "Parish")
```
&ensp;&ensp;  



```{r, results='hide'}
df <- data[-c(1,7:8,12:13,19:20,27,29,31,33,46)]
summary(df)
```
&ensp;&ensp;    

## Data visualization

In this section we are going to see how our features are distributed among the population.
We start with three pie charts that represent three of the categories considered in the census of 2017: the ethnicity, the employment sector and the usual method of transport.

```{r}
attach(data)

#population by ethnicity

df.pop <- data.frame(
  group = c("White", "Hispanic", "Black", "Asian", "Native", "Pacific"),
  value = c(sum(data$TotalPop*White), sum(data$TotalPop*Hispanic), sum(data$TotalPop*Black), sum(data$TotalPop*Asian), sum(data$TotalPop*Native), sum(data$TotalPop*Pacific)))

#from percentages to quantities

df.pop[2]<-round(100*df.pop[2]/sum(df.pop[2]),digits = 2)
df.pop2 <- df.pop %>% 
  mutate(csum = rev(cumsum(rev(value))), 
         pos = value/2 + lead(csum, 1),
         pos = if_else(is.na(pos), value/2, pos))

#population ethnicity pie

ggplot(df.pop, aes(x = "" , y = value, fill = fct_inorder(group))) +
  geom_col(width = 1, color = 1) +
  coord_polar(theta = "y") +
  scale_fill_brewer(palette = "Pastel1") +
  geom_label_repel(data = df.pop2,
                   aes(y = pos, label = paste0(value, "%")),
                   size = 4.5, nudge_x = 1, show.legend = FALSE) +
  guides(fill = guide_legend(title = "Group")) +
  theme_void()
```
&ensp;&ensp;  

As expected the dominant ethnicity is the White one that represent 63% of the total population of the United States. The other mayor ethnicities are Hispanic (18.32%), Black (12.44%) and the Asian (5.44%).

```{r}
#employment sector

df.wk <- data.frame(
  group = c("Professional", "Service", "Office", "Construction", "Production"),
  value = c(sum(TotalPop*Professional), sum(TotalPop*Service), sum(TotalPop*Office), sum(TotalPop*Construction), sum(TotalPop*Production)))

#from percentages to quantities

df.wk[2]<-round(100*df.wk[2]/sum(df.wk[2]),digits = 2)
df.wk2 <- df.wk %>% 
  mutate(csum = rev(cumsum(rev(value))), 
         pos = value/2 + lead(csum, 1),
         pos = if_else(is.na(pos), value/2, pos))

#employment sector pie

ggplot(df.wk, aes(x = "" , y = value, fill = fct_inorder(group))) +
  geom_col(width = 1, color = 1) +
  coord_polar(theta = "y") +
  scale_fill_brewer(palette = "Pastel1") +
  geom_label_repel(data = df.wk2,
                   aes(y = pos, label = paste0(value, "%")),
                   size = 4.5, nudge_x = 1, show.legend = FALSE) +
  guides(fill = guide_legend(title = "Group")) +
  theme_void()
```
&ensp;&ensp; 

We can see that every class has a good representation.  
Later we will see if a certain class of workers prefers a specific party.

```{r}
#transport division

df.tr <- data.frame(
  group = c("Drive", "Carpool", "Transit", "Walk", "OtherTransport"),
  value = c(sum(data$TotalPop*Drive), sum(data$TotalPop*Carpool),
            sum(data$TotalPop*Transit), sum(data$TotalPop*Walk)
            , sum(data$TotalPop*OtherTransp)))

#from percentages to quantities

df.tr[2]<-round(100*df.tr[2]/sum(df.tr[2]),digits = 2)
df.tr2 <- df.tr %>% 
  mutate(csum = rev(cumsum(rev(value))), 
         pos = value/2 + lead(csum, 1),
         pos = if_else(is.na(pos), value/2, pos))

#transport division pie

ggplot(df.tr, aes(x = "" , y = value, fill = fct_inorder(group))) +
  geom_col(width = 1, color = 1) +
  coord_polar(theta = "y") +
  scale_fill_brewer(palette = "Pastel1") +
  geom_label_repel(data = df.tr2,
                   aes(y = pos, label = paste0(value, "%")),
                   size = 4.5, nudge_x = 1, show.legend = FALSE) +
  guides(fill = guide_legend(title = "Group")) +
  theme_void()
```
&ensp;&ensp; 

The transport data is correlated to the states geographical morphology: outside the big cities distances become huge so the use of a car by over the 90% of the population (Drive and Carpool) is justified.


Now we analyse the characteristics of the counties: population and winning party.

```{r}
# distribution of the population along the counties

par(mfrow=c(1, 1))
hist(log10(TotalPop), col='deepskyblue4',breaks = 40, main = 'LogPopulation - distribution Frequency')
```
&ensp;&ensp; 

This histogram represents the size of the counties. We can see that most of the counties have less than 100 000 inhabitants and the counties with over 1 000 000 inhabitants are very few. 

```{r}
#winning party for county in 2020

table(Party20)

```


We know that the election were won by the Democratic party in 2020, but, as we just saw, they won in almost 20% of the counties. This fact suggest that the distribution of the population is not the same between republican counties and democrat counties. Now we are going to investigate this fact. 

```{r}
# distribution of the population along the counties per winning party

boxplot(log(TotalPop)[Party20=='Democrat'], log(TotalPop)[Party20=='Republican'], col=c('blue', 'red'), names=c("Democrats", "Republicans"), ylab = 'Log(TotalPop)')

```
&ensp;&ensp;

As suspected the distribution is not the same; the democratic counties in general counts more inhabitants and also the biggest counties are all democratic.

&ensp;&ensp;

Now we want to understand how the U.S.population is distributed over the territory. With the help of some maps is easier to visualize the geographic areas with more ethnic diversity as well as other data as density of population and unemployment.
We start with the map representing the 2020 elections results and then the others.


```{r, results='hide', warning=FALSE, message= FALSE}

options(repr.plot.width = 17, repr.plot.height = 9)
prov.usa <- map_data("state")
prov.abb <- data.frame(state = state.name, Abb = state.abb)
prov.abb2<-prov.abb
prov.abb2$state<-tolower(prov.abb2$state)
prov.election2020 <- data %>% 
  group_by(state) %>% 
  summarise(votes20_Joe_Biden = sum(votes20_Joe_Biden),
            votes20_Donald_Trump = sum(votes20_Donald_Trump)) %>% 
  ungroup() %>% 
  gather(Candidate, Votes, -state) %>% 
  mutate(Party = factor(if_else(Candidate == "votes20_Joe_Biden", "Democrat", "Republican"))) %>% 
  left_join(prov.abb2) %>% 
  rename("region" = state)
prov.election20202 <- prov.election2020 %>% 
  group_by(region, Abb) %>% 
  summarise(Votes = max(Votes)) %>% 
  ungroup() %>% 
  inner_join(prov.election2020)
prov.election20202<-prov.election20202[,-2]
colnames(prov.abb2)[1]<-"region"
colnames(prov.election20202)[1]<-"Abb"
prov.election20203<-left_join(prov.election20202,prov.abb2)
prov.usa_election2020 <- left_join(prov.usa, prov.election20203) %>% 
  filter(!is.na(Party))
map.us.20<-ggplot(data = prov.usa_election2020, aes(x = long, y = lat),color = prov.usa_election2020$Party)+
  geom_polygon(aes(group = group, fill = Party),color = "gray90", size = 0.1) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) + scale_fill_manual(values = alpha(c("blue2", "red3"), 1))
prov.data2 <- data %>% 
  group_by(state) %>% 
  summarise(Black = sum(Black*TotalPop),
            Hispanic=sum(Hispanic*TotalPop),
            Asian=sum(Asian*TotalPop),
            Unemployment = sum(Unemployment*TotalPop),
            votes16_Hillary_Clinton = sum(votes16_Hillary_Clinton),
            votes16_Donald_Trump = sum(votes16_Donald_Trump),
            TotalPop = sum(TotalPop)) %>% 
  ungroup() %>% 
  rename("region" = state)
prov.data2$Black<- prov.data2$Black/prov.data2$TotalPop
prov.data2$Hispanic<- prov.data2$Hispanic/prov.data2$TotalPop
prov.data2$Asian<- prov.data2$Asian/prov.data2$TotalPop
prov.data2$Unemployment<- prov.data2$Unemployment/prov.data2$TotalPop
colnames(prov.abb2)[1]<-"Abb"
colnames(prov.abb2)[2]<-"region"
prov.data3<-left_join(prov.data2,prov.abb2)
colnames(prov.data3)[1]<-"Abb"
colnames(prov.data3)[9]<-"region"
prov.usa_data <- left_join(prov.usa, prov.data3) 

map.pop<-ggplot(data = prov.usa_data, aes(x = long, y = lat),color = prov.usa_data$TotalPop)+
  geom_polygon(aes(group = group, fill = TotalPop),color = "gray90", size = 0.1) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +scale_fill_gradient(names<-"TotalPop", low="White", high = "deepskyblue4")

map.black<-ggplot(data = prov.usa_data, aes(x = long, y = lat),color = prov.usa_data$Black)+
  geom_polygon(aes(group = group, fill = Black),color = "gray90", size = 0.1) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +scale_fill_gradient(names<-"Black", low="White", high = "Black")

map.hisp<-ggplot(data = prov.usa_data, aes(x = long, y = lat),color = prov.usa_data$Hispanic)+
  geom_polygon(aes(group = group, fill = Hispanic),color = "gray90", size = 0.1) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +scale_fill_gradient(names<-"Hispanic", low="White", high = "coral3")

map.asian<-ggplot(data = prov.usa_data, aes(x = long, y = lat),color = prov.usa_data$Asian)+
  geom_polygon(aes(group = group, fill = Asian),color = "gray90", size = 0.1) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +scale_fill_gradient(names<-"Asian", low="White", high = "darkgoldenrod")

map.unemp<-ggplot(data = prov.usa_data, aes(x = long, y = lat),color = prov.usa_data$Unemployment)+
  geom_polygon(aes(group = group, fill = Unemployment),color = "gray90", size = 0.1) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +scale_fill_gradient(names<-"Unemployment", low="White", high = "brown")


map.us.20
map.pop
map.black
map.hisp
map.asian
map.unemp
```
&ensp;&ensp;

From what we saw until now, we can argue that the number of inhabitants and the presence of different ethnicities are in some form correlated with the political leaning of the state.
Below we display the scatterplot regarding aforementioned data.


```{r}
par(mfrow=c(1,1))
ggplot(data, aes(x=White, y=log(TotalPop),colour=Party20, size=TotalPop )) +
  geom_point(alpha=0.5) +   scale_color_manual(values = alpha(c("blue2", "red3"), 1)) + 
  theme_bw()
ggplot(data, aes(x=Black, y=log(TotalPop),colour=Party20, size=TotalPop )) +
  geom_point(alpha=0.5) +   scale_color_manual(values = alpha(c("blue2", "red3"), 1)) + 
  theme_bw()
ggplot(data, aes(x=Hispanic, y=log(TotalPop),colour=Party20, size=TotalPop )) +
  geom_point(alpha=0.5) +   scale_color_manual(values = alpha(c("blue2", "red3"), 1)) + 
  theme_bw()
ggplot(data, aes(x=Asian, y=log(TotalPop),colour=Party20, size=TotalPop )) +
  geom_point(alpha=0.5) +   scale_color_manual(values = alpha(c("blue2", "red3"), 1)) + 
  theme_bw()
```
&ensp;&ensp;

All the scatterplots indicate a similar relation between ethnicity, population and elections results: Republicans win in small counties with a low ethnic diversity (majority White), while Democrats win in large counties or where the ethnic diversity is high (with the exceptions of Hispanic population that is less incline to vote the Democratic party for historical reasons).

This analysis suggest that where the black community is more present the Democratic party should win but the map show that the states with a higher percentage of Black ethnicity (the southern-eastern states) voted Republican. Now we will look at this phenomenon at a county-level. 

```{r}
#Counties

data %>% 
  group_by(state) %>% 
  mutate(fips = fips(state = first(state), county = county)) -> FIPS.data

plot_usmap(data = FIPS.data, values = "Party20", include = c("AL", "AR", "GA", "LA", "MS", "NC", "SC", "TN")) + 
  scale_fill_manual(values = (c("blue2", "red3")), name = "Winners (2020)") + 
  theme(legend.position = "right")

plot_usmap(data = FIPS.data, values = "Black", include = c("AL", "AR", "GA", "LA", "MS", "NC", "SC", "TN")) + 
  scale_fill_continuous(low = "white", high = "black", name = "Black (2017)", label = scales::comma) + 
  theme(legend.position = "right")
```
&ensp;&ensp;

From these two maps is easy to see that our initial intuition was right: the counties with a higher presence of Black have a democratic leaning, but many other counties that have a more modest ethnic representation voted Republican. The state results are based on all citizens votes so it can be a bit misleading to use just a state result to make assumption on its counties.
Now we are going to analyze another extreme case: the state of New York, were there are a small number of extremely populated areas (mainly the city of New York) and a lot of counties that count very few citizens.

```{r}

plot_usmap(data = FIPS.data, values = "Party20", include = "NY") + 
  scale_fill_manual(values = (c("blue2", "red3")), name = "Winners (2020)") + 
  theme(legend.position = "right")
table(Party20[state=='NY'])
plot_usmap(data = FIPS.data, values = "TotalPop", include = "NY") + 
  scale_fill_continuous(low = "white", high = "deepskyblue4", name = "Population (2017)", label = scales::comma) + 
  theme(legend.position = "right")

```
&ensp;&ensp;

In the state of New York the Democratic party won with a big lead of votes, but watching the results map it would appear that republicans won (in fact they got 49 counties out of 62). This is due to the amount of population concentrated only in few counties: urbanized areas are extremely populated so they can easily overturn the outcome in a state. For example the population in New York City alone is around 40% of the entire state and this allowed the victory of the Democratic Party even though they lost in 80% of the counties.



-------------------------------REGRESSIONS--------------------------------------

```{r,  warning=FALSE}
#regression_here

#correggere "v" e data <- df
v<-c(percentage20_Joe_Biden>percentage20_Donald_Trump)
regfit.full <- regsubsets(v~., data=data[-c(1:13,19:20,27,29,31,33,46,52,53)], really.big = T,nvmax = 34)
reg.summary <- summary(regfit.full)
reg.summary$outmat
par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted Rsq",type="l")
which.max(reg.summary$adjr2)
points(which.max(reg.summary$adjr2),reg.summary$adjr2[which.max(reg.summary$adjr2)], col="red",cex=2,pch=20)
# Mallow's Cp with its smallest value
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
which.min(reg.summary$cp)
points(which.min(reg.summary$cp),reg.summary$cp[which.min(reg.summary$cp)],col="red",cex=2,pch=20)
# BIC with its smallest value
plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
which.min(reg.summary$bic)
points(which.min(reg.summary$bic),reg.summary$bic[which.min(reg.summary$bic)],col="red",cex=2,pch=20)
par(mfrow=c(1,1))
plot(regfit.full,scale="r2", col="#006699")
plot(regfit.full,scale="adjr2", col="#006699")
plot(regfit.full,scale="Cp", col="#006699")
plot(regfit.full,scale="bic", col="#006699")


```
&ensp;&ensp; 

```{r,  warning=FALSE}
#ridge regression
par(mfrow=c(1,2))

X <- model.matrix(percentage20_Joe_Biden~.,data[-c(1:13,19:20,27,29,31,33,52,53)])
y<-percentage20_Joe_Biden
grid.ridge<-10^seq(2,-4,length=100)
ridge.mod<-glmnet(X,y,alpha = 0,lambda = grid.ridge)
plot(ridge.mod, xvar="lambda")



train <- sample(1:nrow(X),nrow(X)/2)
test<-(-train)
y.test<-y[test]
ridge.pred<-predict(ridge.mod,s=4,newx = X[test,],type = "response")
mean((ridge.pred-y.test)^2)


#lm(percentage20_Joe_Biden~.,data[-c(1:13,19:20,27,29,31,33,52,53)],subset = train)
predict(ridge.mod,s=0,exact = TRUE,type = "coefficients",x=X[train,],y=y[train])[1:20,]


# use cross-validation to choose the value of lambda 

cv.out <- cv.glmnet(X[train, ], y[train], alpha = 0,lambda = grid.ridge, nfold=10)

cv.out$cvm
summary(cv.out$lambda)

plot(cv.out)
i.bestlam <- which.min(cv.out$cvm)
i.bestlam 
bestlam <- cv.out$lambda[i.bestlam]
bestlam
cv.out$cvm[i.bestlam]


ridge.pred <- predict(ridge.mod, s = bestlam,
                      newx = X[test, ])
mean((ridge.pred - y.test)^2)


#lasso regression code
X <- model.matrix(percentage20_Joe_Biden~.,data[-c(1:13,19:20,27,29,31,33,52,53)])
y<-percentage20_Joe_Biden
grid.lasso<-10^seq(-0.5,-6,length=100)
lasso.mod<-glmnet(X,y,alpha = 1,lambda = grid.lasso)
plot(lasso.mod, xvar="lambda")



train <- sample(1:nrow(X),nrow(X)/2)
test<-(-train)
y.test<-y[test]
lasso.pred<-predict(lasso.mod,s=4,newx = X[test,],type = "response")
mean((lasso.pred-y.test)^2)


predict(ridge.mod,s=0,exact = TRUE,type = "coefficients",x=X[train,],y=y[train])[1:20,]


# use cross-validation to choose the value of lambda 

cv.out.lasso <- cv.glmnet(X[train, ], y[train], alpha = 1,lambda = grid.lasso, nfold=10)

cv.out.lasso$lambda
summary(cv.out.lasso$lambda)

plot(cv.out.lasso)
i.bestlam.lasso <- which.min(cv.out.lasso$cvm)
i.bestlam.lasso
bestlam.lasso <- cv.out.lasso$lambda[i.bestlam.lasso]
bestlam.lasso

cv.out.lasso$cvm[i.bestlam.lasso]

lasso.pred <- predict(lasso.mod, s = bestlam.lasso,
                      newx = X[test, ])
mean((lasso.pred - y.test)^2)
par(mfrow=c(1,1))

```
&ensp;&ensp; 

```{r, warning=FALSE}
###############################
# LOGISTIC CLASSIFIER
##############################

# logistic classifier
mod.out <- glm(Party20 ~. , data=data[-c(1:13,19:20,27,29,31,33,52,53)], family = binomial)
logistic.prob <- predict(mod.out, type="response")

logistic.pred <- rep("Democrat", 3046)
logistic.pred[logistic.prob>0.5] <- "Republican"


# decision boundary
summary(mod.out)
beta <- coefficients(mod.out)

# confusion matrix
table(logistic.pred, Party20)

# trivial classifier 
trivial.pred <- rep("Democrat", 3046)
table(trivial.pred, Party20)

# threshold=0.2
logistic.pred <- rep("Democrat", 3046)
logistic.pred[logistic.prob>0.2] <- "Republican"
table(logistic.pred, Party20)

CM <- table(logistic.pred, Party20)

# rearrange rows and columns to match 
# the confusion matrix on the slides
CM <- CM[2:1, 2:1]

# add margins to the confusion matrix
CM <- addmargins(CM, margin = c(1, 2))
CM

# FPR: False Positive Rate = FP/N
#################################

fpr <- CM["Republican", "Democrat"]/CM["Sum", "Democrat"]   
fpr 

# TNR : True negative rate = TN/N = (1 - FPR)  
# TNR = Specificity, which refers to Pr(negative | negative)

specif <- 1 - fpr
specif

# TPR: True Positive Rate = TP/P
# TPR = Sensitivity, which refers to Pr(positive | positive)
############################################################

tpr <- CM["Republican", "Republican"]/CM["Sum", "Republican"]   
tpr 

# ROC curve 
# If labels are "0" and "1" then "0"=negative and "1"=positive
# levels = negative (0's) as first element and positive (1's) as second
roc.out <- roc(Party20, logistic.prob, levels=c("Democrat", "Republican"))

# different ways of plotting the ROC curve (check labels of the axes)
plot(roc.out,  print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")
plot(roc.out, legacy.axes=TRUE)
plot(roc.out)

auc(roc.out)
plot(roc.out,  print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")

# specificity and sensitivity for a given threshold
coords(roc.out, 0.2)

# threshold that maximises the sum of sensitivity and specificity
coords(roc.out, "best")

```
