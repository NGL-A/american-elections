---
title: "Elections, COVID and Demographic Data"
author: "A. Bissacco, F. Sartori, M. Sboarina"
date: "21/06/2022"
output:
  #word_document: default
  #html_document: default
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(corrplot)
library(ggplot2)
library(ggrepel)
library(glmnet)
library(leaps)
library(mapproj)
library(maps)
library(pROC)
library(ROCR)
library(tidyverse)
library(usmap)
```

## US Election
Once every 4 years one of the most relevant political event in the world takes place: the election of the US President. After months, even years, of preparations (campaigns, public speeches, sponsors...) the eyes of the entire world are looking to the results of those elections, to the person who will be leading the "free world" in the near future.

Our project wants to focus on what might affect citizens vote: if the geographical area is relevant, if the type of work can change the preference, if one ethnicity is more likely to vote a Party over the other and if there is any strong correlation between these variables.  
We found our dataset on [Kaggle](https://www.kaggle.com/datasets/etsc9287/2020-general-election-polls). It contains the 2017 demographic and economic situation of each American county, the numbers of 2016 and 2020 US Elections and COVID report until the 1st of November 2020 (few days before the elections).

The USA are the only country in the world applying an Electoral College system to pick their President. This mechanism gives a weight to each state according to its population: the more it is populated the more significant it will be when voting. When a party wins a state it will get **every** Electoral College regardless of the victory margin. Even though every 10 years the Government updates these weights, many states (typically smaller ones) have much more representation per citizen than others. For this reason candidate campaigns focus on these states and on gaining the support of the ones which are not historically biased on a specific party (*swing states*).

### Clean dataset
We start our analysis by removing NA values. We delete 1821 rows that are mostly counties which don't present all of the features. Counties that changed name have been classified both with the original name and the new name, but the latter contains only COVID and 2020 election data. This lead us to 3046 counties which is close to their real number of 3142^[Alaska only had the data for 2020 elections so all its boroughs had been removed.].

```{r, results='hide'}
data_uncleaned <- read.csv("county_statistics.csv", sep = ",", header = TRUE)
data <- na.omit(data_uncleaned)

```
&ensp;&ensp;  


The dataset is not consistent in the percentage notation so we turn percentages (coming from Census 2017) to relative numbers in [0,1].  

```{r}
data[,c(21:26,32:45,47:51)] <- (data[,c(21:26,32:45,47:51)]/100)
```
&ensp;&ensp;  


Our dataset contains the following 51 variables:

&ensp;&ensp;1. X: county index (from 1 to 4954)  
&ensp;&ensp;2. County: county name  
&ensp;&ensp;3. State: Two-Letter State Abbreviation  
&ensp;&ensp;4. percentage16_Donald_Trump: percentage of votes Donald Trump got in 2016  
&ensp;&ensp;5. percentage16_Hillary_Clinton: percentage of votes Hilary Clinton got in 2016  
&ensp;&ensp;6. total_votes16: total number of votes in 2016  
&ensp;&ensp;7. votes16_Donald_Trump: number of votes Donald Trump got in 2016  
&ensp;&ensp;8. votes16_Hillary_Clinton: number of votes Hilary Clinton got in 2016  
&ensp;&ensp;9. percentage20_Donald_Trump: percentage of votes Donald Trump got in 2020  
&ensp;&ensp;10. percentage20_Joe_Biden: percentage of votes Joe Biden got in 2020  
&ensp;&ensp;11. total_votes20: total number of votes in 2020  
&ensp;&ensp;12. votes20_Donald_Trump: number of votes Donald Trump got in 2020  
&ensp;&ensp;13. votes20_Joe_Biden: number of votes Joe Biden got in 2020  
&ensp;&ensp;14. lat: latitude coordinates  
&ensp;&ensp;15. long: longitude coordinates  
&ensp;&ensp;16. cases: COVID-19 cases  
&ensp;&ensp;17. deaths: COVID-19 deaths  
&ensp;&ensp;18. TotalPop: Total population  
&ensp;&ensp;19. Men: number of men  
&ensp;&ensp;20. Women: number of women  
&ensp;&ensp;21. Hispanic: percent of population that is Hispanic/Latino  
&ensp;&ensp;22. White: percent of population that is white  
&ensp;&ensp;23. Black: percent of population that is black  
&ensp;&ensp;24. Native: percent of population that is Native American or Native Alaskan  
&ensp;&ensp;25. Asian: percent of population that is Asian  
&ensp;&ensp;26. Pacific: percent of population that is Native Hawaiian or Pacific Islander  
&ensp;&ensp;27. VotingAgeCitizen: number of citizens above voting age  
&ensp;&ensp;28. Income: median household income  
&ensp;&ensp;29. IncomeErr: median household income error  
&ensp;&ensp;30. IncomePerCap: income per capita  
&ensp;&ensp;31. IncomePerCapErr: income per capita error  
&ensp;&ensp;32. Poverty: percent of population under poverty level  
&ensp;&ensp;33. ChildPoverty percent of children under poverty level  
&ensp;&ensp;34. Professional: percent of population employed in management, business, science and arts  
&ensp;&ensp;35. Service: percent of population employed in service jobs  
&ensp;&ensp;36. Office: percent of population employed in sales and office jobs  
&ensp;&ensp;37. Construction: percent of population employed in natural resources, construction and maintenance  
&ensp;&ensp;38. Production: percent of population employed in production, transportation and material movement  
&ensp;&ensp;39. Drive: percent commuting alone in a car, van, or truck  
&ensp;&ensp;40. Carpool: percent carpooling in a car, van, or truck  
&ensp;&ensp;41. Transit: percent commuting on public transportation  
&ensp;&ensp;42. Walk: percent walking to work OtherTransp percent commuting via other means  
&ensp;&ensp;43. OtherTransp: percent using other type of transportation  
&ensp;&ensp;44. WorkAtHome: percent of population working at home  
&ensp;&ensp;45. MeanCommute: mean commute time (minutes)  
&ensp;&ensp;46. Employed: percent of population employed (16+)  
&ensp;&ensp;47. PrivateWork: percent of population employed in private industry  
&ensp;&ensp;48. PublicWork: percent of population employed in public jobs  
&ensp;&ensp;49. SelfEmployed: percent of population self-employed  
&ensp;&ensp;50. FamilyWork: percent of population in unpaid family work  
&ensp;&ensp;51. Unemployment: unemployment rate (percent)  
&ensp;&ensp;  

We want to see between which variables exists a deep correlation through a correlation matrix^[We changed columns "percentageXX_Candidate" into "pCandidateXX" and "votesXX_Candidate" into "CandidateXX" to obtain a clearer plot.].

```{r, results='hide'}
data.corr <- data
names(data.corr)[names(data.corr) == "percentage16_Donald_Trump"] <- "pTrump16"
names(data.corr)[names(data.corr) == "percentage16_Hillary_Clinton"] <- "pClinton16"
names(data.corr)[names(data.corr) == "percentage20_Donald_Trump"] <- "pTrump20"
names(data.corr)[names(data.corr) == "percentage20_Joe_Biden"] <- "pBiden20"
names(data.corr)[names(data.corr) == "votes16_Donald_Trump"] <- "Trump16"
names(data.corr)[names(data.corr) == "votes16_Hillary_Clinton"] <- "Clinton16"
names(data.corr)[names(data.corr) == "votes20_Donald_Trump"] <- "Trump20"
names(data.corr)[names(data.corr) == "votes20_Joe_Biden"] <- "Biden20"
cor <- cor(data.corr[4:51])
corrplot(cor, type = 'upper', tl.cex = 0.5)

```

We discover that many variables are deeply correlated because they are absolute frequency values and basically a linear transformation of another column. 
In fact the matrix proves us that, since females and males are equally distributed in almost every county, one gender doesn't affect any of the variables more than the other. Moreover we acknowledge that "ChildPoverty" behave like "Poverty", so we keep only the latter; while we chose "TotalPop" to represent "Men" and "Women", but also "Emplyed" and "VotingAgeCitizen". "IncomeErr" and "IncomePerCapErr" are grouped respectively with their counterpart "Income" and "IncomePerCap". "votesXX_Candidate" have been considered together with "percentageXX_Candidate".  
When managing absolute frequencies we need to remember that they are all correlated to the whole population. In order to obtain clearer comparison between the counties statistics we decided upon working though relative values over absolute.  

So we decided to maintain these:  
&ensp;&ensp;1. County  
&ensp;&ensp;2. State  
&ensp;&ensp;3. percentage16_Donald_Trump   
&ensp;&ensp;4. percentage16_Hillary_Clinton  
&ensp;&ensp;5. total_votes16   
&ensp;&ensp;6. percentage20_Donald_Trump  
&ensp;&ensp;7. percentage20_Joe_Biden   
&ensp;&ensp;8. total_votes20  
&ensp;&ensp;9. lat  
&ensp;&ensp;10. long  
&ensp;&ensp;11. cases  
&ensp;&ensp;12. deaths  
&ensp;&ensp;13. TotalPop  
&ensp;&ensp;14. Hispanic  
&ensp;&ensp;15. White  
&ensp;&ensp;16. Black  
&ensp;&ensp;17. Native   
&ensp;&ensp;18. Asian  
&ensp;&ensp;19. Pacific  
&ensp;&ensp;20. Income  
&ensp;&ensp;21. IncomePerCap  
&ensp;&ensp;22. Poverty  
&ensp;&ensp;23. Professional  
&ensp;&ensp;24. Service  
&ensp;&ensp;25. Office  
&ensp;&ensp;26. Construction  
&ensp;&ensp;27. Production  
&ensp;&ensp;28. Drive  
&ensp;&ensp;29. Carpool  
&ensp;&ensp;30. Transit  
&ensp;&ensp;31. Walk  
&ensp;&ensp;32. OtherTransp  
&ensp;&ensp;33. WorkAtHome  
&ensp;&ensp;34. MeanCommute  
&ensp;&ensp;35. PrivateWork  
&ensp;&ensp;36. PublicWork  
&ensp;&ensp;37. SelfEmployed  
&ensp;&ensp;38. FamilyWork  
&ensp;&ensp;39. Unemployment  

To obtain a clearer view of the election results we add two new columns with each county winner Party in 2016 and 2020.

```{r, results='hide'}
data$Party16 = factor(if_else(data$percentage16_Hillary_Clinton >= data$percentage16_Donald_Trump, "Democrat", "Republican"))
data$Party20 = factor(if_else(data$percentage20_Joe_Biden >= data$percentage20_Donald_Trump, "Democrat", "Republican"))
```

Since later on we will have to analyze and plot the situation in some counties in the USA, we need to fix Louisiana's ones. The state is in fact, the only one in the country which divides its territory in parishes and our dataset is missing the suffix "Parish" in the name of each subdivision.

```{r, results='hide'}
data$county[data$state == "LA"] = paste(data$county[data$state == "LA"], "Parish")
```



&ensp;&ensp;  

## Data visualization

In this section we see how our features are distributed among the population.
We start with three pie charts that represent three of the categories considered in the census of 2017: the ethnicity, the employment sector and the usual method of transport.

```{r}
attach(data)
#population by ethnicity
df.pop <- data.frame(
  group = c("White", "Hispanic", "Black", "Asian", "Native", "Pacific"),
  value = c(sum(data$TotalPop*White), sum(data$TotalPop*Hispanic), sum(data$TotalPop*Black), sum(data$TotalPop*Asian), sum(data$TotalPop*Native), sum(data$TotalPop*Pacific)))
#from percentages to quantities
df.pop[2]<-round(100*df.pop[2]/sum(df.pop[2]),digits = 2)
df.pop2 <- df.pop %>% 
  mutate(csum = rev(cumsum(rev(value))), 
         pos = value/2 + lead(csum, 1),
         pos = if_else(is.na(pos), value/2, pos))
#population ethnicity pie
ggplot(df.pop, aes(x = "" , y = value, fill = fct_inorder(group))) +
  geom_col(width = 1, color = 1) +
  coord_polar(theta = "y") +
  scale_fill_brewer(palette = "Pastel1") +
  geom_label_repel(data = df.pop2,
                   aes(y = pos, label = paste0(value, "%")),
                   size = 4.5, nudge_x = 1, show.legend = FALSE) +
  guides(fill = guide_legend(title = "Group")) +
  theme_void()
```
&ensp;&ensp;  

As expected the dominant ethnicity is the White one that represent 63% of the total population of the United States. The other mayor ethnicities are Hispanic (18.32%), Black (12.44%) and Asian (5.44%).

```{r}
#employment sector
df.wk <- data.frame(
  group = c("Professional", "Service", "Office", "Construction", "Production"),
  value = c(sum(TotalPop*Professional), sum(TotalPop*Service), sum(TotalPop*Office), sum(TotalPop*Construction), sum(TotalPop*Production)))
#from percentages to quantities
df.wk[2]<-round(100*df.wk[2]/sum(df.wk[2]),digits = 2)
df.wk2 <- df.wk %>% 
  mutate(csum = rev(cumsum(rev(value))), 
         pos = value/2 + lead(csum, 1),
         pos = if_else(is.na(pos), value/2, pos))
#employment sector pie
ggplot(df.wk, aes(x = "" , y = value, fill = fct_inorder(group))) +
  geom_col(width = 1, color = 1) +
  coord_polar(theta = "y") +
  scale_fill_brewer(palette = "Pastel1") +
  geom_label_repel(data = df.wk2,
                   aes(y = pos, label = paste0(value, "%")),
                   size = 4.5, nudge_x = 1, show.legend = FALSE) +
  guides(fill = guide_legend(title = "Group")) +
  theme_void()
```
&ensp;&ensp; 

We can see that every class has a good representation.  
Later we will see if a certain class of workers prefers a specific party.

```{r}
#transport division
df.tr <- data.frame(
  group = c("Drive", "Carpool", "Transit", "Walk", "OtherTransport"),
  value = c(sum(data$TotalPop*Drive), sum(data$TotalPop*Carpool),
            sum(data$TotalPop*Transit), sum(data$TotalPop*Walk)
            , sum(data$TotalPop*OtherTransp)))
#from percentages to quantities
df.tr[2]<-round(100*df.tr[2]/sum(df.tr[2]),digits = 2)
df.tr2 <- df.tr %>% 
  mutate(csum = rev(cumsum(rev(value))), 
         pos = value/2 + lead(csum, 1),
         pos = if_else(is.na(pos), value/2, pos))
#transport division pie
ggplot(df.tr, aes(x = "" , y = value, fill = fct_inorder(group))) +
  geom_col(width = 1, color = 1) +
  coord_polar(theta = "y") +
  scale_fill_brewer(palette = "Pastel1") +
  geom_label_repel(data = df.tr2,
                   aes(y = pos, label = paste0(value, "%")),
                   size = 4.5, nudge_x = 1, show.legend = FALSE) +
  guides(fill = guide_legend(title = "Group")) +
  theme_void()
```
&ensp;&ensp; 

The transport data is correlated to the states geographical morphology: outside the big cities distances become huge so the use of a car by over the 90% of the population (Drive and Carpool) is justified.


Now we analyse the characteristics of the counties: population and winning party.

```{r}
# distribution of the population along the counties
par(mfrow=c(1, 1))
hist(log10(TotalPop), col='deepskyblue4',breaks = 40, main = 'LogPopulation - distribution Frequency')
```
&ensp;&ensp; 

This histogram represents the size of the counties. We can see that most of the counties have less than 100 000 inhabitants and the counties with over 1 000 000 inhabitants are very few. 

```{r}
#winning party for county in 2020
table(Party20)
```


We know that the election were won by the Democratic party in 2020, but, as we just saw, they won in almost 20% of the counties. This fact suggest that the distribution of the population is not the same between republican counties and democrat counties. Now we are going to investigate this fact. 

```{r}
# distribution of the population along the counties per winning party
boxplot(log(TotalPop)[Party20=='Democrat'], log(TotalPop)[Party20=='Republican'], col=c('blue', 'red'), names=c("Democrats", "Republicans"), ylab = 'Log(TotalPop)')
```
&ensp;&ensp;

As suspected the distribution is not the same; the democratic counties in general count more inhabitants and the biggest counties are all democratic.

&ensp;&ensp;

Now we want to understand how the U.S.population is distributed over the territory. With the help of some maps is easier to visualize the geographic areas with more ethnic diversity as well as other data as density of population and unemployment.
We start with the map representing the 2020 elections results and then the others.


```{r, results='hide', warning=FALSE, message= FALSE}
options(repr.plot.width = 17, repr.plot.height = 9)
prov.usa <- map_data("state")
prov.abb <- data.frame(state = state.name, Abb = state.abb)
prov.abb2<-prov.abb
prov.abb2$state<-tolower(prov.abb2$state)
prov.election2020 <- data %>% 
  group_by(state) %>% 
  summarise(votes20_Joe_Biden = sum(votes20_Joe_Biden),
            votes20_Donald_Trump = sum(votes20_Donald_Trump)) %>% 
  ungroup() %>% 
  gather(Candidate, Votes, -state) %>% 
  mutate(Party = factor(if_else(Candidate == "votes20_Joe_Biden", "Democrat", "Republican"))) %>% 
  left_join(prov.abb2) %>% 
  rename("region" = state)
prov.election20202 <- prov.election2020 %>% 
  group_by(region, Abb) %>% 
  summarise(Votes = max(Votes)) %>% 
  ungroup() %>% 
  inner_join(prov.election2020)
prov.election20202<-prov.election20202[,-2]
colnames(prov.abb2)[1]<-"region"
colnames(prov.election20202)[1]<-"Abb"
prov.election20203<-left_join(prov.election20202,prov.abb2)
prov.usa_election2020 <- left_join(prov.usa, prov.election20203) %>% 
  filter(!is.na(Party))
map.us.20<-ggplot(data = prov.usa_election2020, aes(x = long, y = lat),color = prov.usa_election2020$Party)+
  geom_polygon(aes(group = group, fill = Party),color = "gray90", size = 0.1) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) + scale_fill_manual(values = alpha(c("blue2", "red3"), 1))
prov.data2 <- data %>% 
  group_by(state) %>% 
  summarise(Black = sum(Black*TotalPop),
            Hispanic=sum(Hispanic*TotalPop),
            Asian=sum(Asian*TotalPop),
            Unemployment = sum(Unemployment*TotalPop),
            votes16_Hillary_Clinton = sum(votes16_Hillary_Clinton),
            votes16_Donald_Trump = sum(votes16_Donald_Trump),
            TotalPop = sum(TotalPop)) %>% 
  ungroup() %>% 
  rename("region" = state)
prov.data2$Black<- prov.data2$Black/prov.data2$TotalPop
prov.data2$Hispanic<- prov.data2$Hispanic/prov.data2$TotalPop
prov.data2$Asian<- prov.data2$Asian/prov.data2$TotalPop
prov.data2$Unemployment<- prov.data2$Unemployment/prov.data2$TotalPop
colnames(prov.abb2)[1]<-"Abb"
colnames(prov.abb2)[2]<-"region"
prov.data3<-left_join(prov.data2,prov.abb2)
colnames(prov.data3)[1]<-"Abb"
colnames(prov.data3)[9]<-"region"
prov.usa_data <- left_join(prov.usa, prov.data3) 
map.pop<-ggplot(data = prov.usa_data, aes(x = long, y = lat),color = prov.usa_data$TotalPop)+
  geom_polygon(aes(group = group, fill = TotalPop),color = "gray90", size = 0.1) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +scale_fill_gradient(names<-"TotalPop", low="White", high = "deepskyblue4")
map.black<-ggplot(data = prov.usa_data, aes(x = long, y = lat),color = prov.usa_data$Black)+
  geom_polygon(aes(group = group, fill = Black),color = "gray90", size = 0.1) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +scale_fill_gradient(names<-"Black", low="White", high = "Black")
map.hisp<-ggplot(data = prov.usa_data, aes(x = long, y = lat),color = prov.usa_data$Hispanic)+
  geom_polygon(aes(group = group, fill = Hispanic),color = "gray90", size = 0.1) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +scale_fill_gradient(names<-"Hispanic", low="White", high = "coral3")
map.asian<-ggplot(data = prov.usa_data, aes(x = long, y = lat),color = prov.usa_data$Asian)+
  geom_polygon(aes(group = group, fill = Asian),color = "gray90", size = 0.1) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +scale_fill_gradient(names<-"Asian", low="White", high = "darkgoldenrod")
map.unemp<-ggplot(data = prov.usa_data, aes(x = long, y = lat),color = prov.usa_data$Unemployment)+
  geom_polygon(aes(group = group, fill = Unemployment),color = "gray90", size = 0.1) +
  coord_map(projection = "albers", lat0 = 39, lat1 = 45) +scale_fill_gradient(names<-"Unemployment", low="White", high = "brown")
map.us.20
map.pop
map.black
map.hisp
map.asian
map.unemp
```
&ensp;&ensp;

From what we saw until now, we can argue that the number of inhabitants and the presence of different ethnicities are in some form correlated with the political leaning of the state.
Below we display the scatterplot regarding aforementioned data.


```{r}
par(mfrow=c(1,1))
ggplot(data, aes(x=White, y=log(TotalPop),colour=Party20, size=TotalPop )) +
  geom_point(alpha=0.5) +   scale_color_manual(values = alpha(c("blue2", "red3"), 1)) + 
  theme_bw()
ggplot(data, aes(x=Black, y=log(TotalPop),colour=Party20, size=TotalPop )) +
  geom_point(alpha=0.5) +   scale_color_manual(values = alpha(c("blue2", "red3"), 1)) + 
  theme_bw()
ggplot(data, aes(x=Hispanic, y=log(TotalPop),colour=Party20, size=TotalPop )) +
  geom_point(alpha=0.5) +   scale_color_manual(values = alpha(c("blue2", "red3"), 1)) + 
  theme_bw()
ggplot(data, aes(x=Asian, y=log(TotalPop),colour=Party20, size=TotalPop )) +
  geom_point(alpha=0.5) +   scale_color_manual(values = alpha(c("blue2", "red3"), 1)) + 
  theme_bw()
```
&ensp;&ensp;

All the scatterplots indicate a similar relation between ethnicity, population and elections results: Republicans win in small counties with a low ethnic diversity (majority White), while Democrats win in large counties or where the ethnic diversity is high (with the exceptions of Hispanic population that is less incline to vote the Democratic party for historical reasons).
Now to verify if our assumptions are right we perform a t-test to see if democratic counties and republican counties behaves the same.
```{r}
# Population
t.test(TotalPop[c(Party20=='Republican')],TotalPop[c(Party20=='Democrat')])
# Black 
t.test(Black[c(Party20=='Republican')],Black[c(Party20=='Democrat')])
# Hispanic
t.test(Hispanic[c(Party20=='Republican')],Hispanic[c(Party20=='Democrat')])
# Asian
t.test(Asian[c(Party20=='Republican')],Asian[c(Party20=='Democrat')])
```
We can see that all p-values are very small so we have strong evidence against the hypothesis that the two factions behaves the same.

This analysis suggests that where the black community is more present the Democratic party should win but the map shows that the states with a higher percentage of Black ethnicity (the south-eastern states) voted Republican. Now we will look at this phenomenon at a county-level. 


```{r}
#Counties
data %>% 
  group_by(state) %>% 
  mutate(fips = fips(state = first(state), county = county)) -> FIPS.data
plot_usmap(data = FIPS.data, values = "Party20", include = c("AL", "AR", "GA", "LA", "MS", "NC", "SC", "TN")) + 
  scale_fill_manual(values = (c("blue2", "red3")), name = "Winners (2020)") + 
  theme(legend.position = "right")
plot_usmap(data = FIPS.data, values = "Black", include = c("AL", "AR", "GA", "LA", "MS", "NC", "SC", "TN")) + 
  scale_fill_continuous(low = "white", high = "black", name = "Black (2017)", label = scales::comma) + 
  theme(legend.position = "right")
```
&ensp;&ensp;

From these two maps is easy to see that our initial intuition was right: the counties with a higher Black ethnicity presence have a democratic leaning, but many other counties that have a more modest ethnic representation voted Republican. The state results are based on all citizens votes so it can be a bit misleading to use just a state result to make assumption on its counties.
Now we are going to analyze another extreme case: the state of New York, were there are a small number of extremely populated areas (mainly the city of New York) and a lot of counties that count very few citizens.

```{r}
plot_usmap(data = FIPS.data, values = "Party20", include = "NY") + 
  scale_fill_manual(values = (c("blue2", "red3")), name = "Winners (2020)") + 
  theme(legend.position = "right")
table(Party20[state=='NY'])
plot_usmap(data = FIPS.data, values = "TotalPop", include = "NY") + 
  scale_fill_continuous(low = "white", high = "deepskyblue4", name = "Population (2017)", label = scales::comma) + 
  theme(legend.position = "right")
```
&ensp;&ensp;

In the state of New York the Democratic party won with a big lead of votes, but watching the results map it would appear that republicans won (in fact they got 49 counties out of 62). This is due to the amount of population concentrated only in a few counties: urbanized areas are extremely populated so they can easily overturn the outcome in a state. For example the population in New York City alone is around 40% of the entire state and this allowed the victory of the Democratic Party even though they lost in 80% of the counties.

-------------------------------REGRESSIONS--------------------------------------

## Regression Analysis

Throughout this paragraph we are going to operate a Regression analysis on different regression models. We will also introduce concepts as Variable Selection and Regularization.
The aim is to create a model which is able to predict the outcome of the 2020 US elections. Therefore the object of our research is the variable "Party20" which we created to understand the party who won in each county.

## Logistic Regression

Let us introduce this Regression analysis by studying the "Logisitc Regression" and the information it gives us about our problem. The code below estimates a logistic regression model using the glm (generalized linear model) function. The output returns the beta coefficient for each variable, along with its standard error, z value and p value. The '*' symbols are an indication of importance for each variable. We are later going to expand this concept with the "Best Subset method", studying the variables importance relatively to the number of parameters we wish to have in our model.


```{r, warning=FALSE}
df <- data[-c(1:13,19:20,27,29,31,33,46,52:53)]
mod.out <- glm(Party20 ~. , data=df, family = binomial)
logistic.prob <- predict(mod.out, type="response")
logistic.pred <- rep("Democrat", 3046)
logistic.pred[logistic.prob>0.5] <- "Republican"
summary(mod.out)
beta <- coefficients(mod.out)
```
&ensp;&ensp; 

From this tables we see the Confusion Matrix with a threshold of 0.5, the Confusion Matrix with a threshold of 0.2 and the Trivial Classifier's output. Let us analyze the false positive, the true negative and the true positive rates. We will further investigate with a Roc Curve study.

The 0.5 threshold is a standard choice and works quite well with a 0.7425532 specificity and a 0.9751553 sensitivity value.

&ensp;&ensp; 

```{r}
# confusion matrix threshold=0.5
table(logistic.pred, Party20)

CM <- table(logistic.pred, Party20)
CM <- CM[2:1, 2:1]
CM <- addmargins(CM, margin = c(1, 2))
CM

# false positive rate = FP/N
fpr <- CM["Republican", "Democrat"]/CM["Sum", "Democrat"]   
fpr 

# true negative rate = TN/N Specificity
specif <- 1 - fpr
specif

# true positive rate = TP/P Sensitivity
tpr <- CM["Republican", "Republican"]/CM["Sum", "Republican"] 
tpr 

```


&ensp;&ensp; 

On the other hand, the 0.2 threshold has a much lower specificity (set at 0.5702128) which is an index of great missclassification for the true negative rate.

&ensp;&ensp; 


```{r}
# confusion matrix threshold=0.2
logistic.pred <- rep("Democrat", 3046)
logistic.pred[logistic.prob>0.2] <- "Republican"
table(logistic.pred, Party20)

CM <- table(logistic.pred, Party20)
CM <- CM[2:1, 2:1]
CM <- addmargins(CM, margin = c(1, 2))
CM

# trivial classifier 
trivial.pred <- rep("Democrat", 3046)
table(trivial.pred, Party20)

# false positive rate = FP/N
fpr <- CM["Republican", "Democrat"]/CM["Sum", "Democrat"]   
fpr 

# true negative rate = TN/N Specificity
specif <- 1 - fpr
specif

# true positive rate = TP/P Sensitivity
tpr <- CM["Republican", "Republican"]/CM["Sum", "Republican"] 
tpr 


```

&ensp;&ensp; 

The Roc curve is a graphical representation of what happens in the previous tables. The AUC (Area Under Curve) always has good values (close to 1) and is correctly shaped, meaning our predictions are fairly accurate. We also find the best threshold value to be at 0.8739469.

&ensp;&ensp; 

```{r}

# Roc curve 
roc.out <- roc(Party20, logistic.prob, levels=c("Democrat", "Republican"))

plot(roc.out,  print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")

# specificity and sensitivity for threshold of 0.5
coords(roc.out, 0.5)

# specificity and sensitivity for threshold of 0.2
coords(roc.out, 0.2)

# threshold that maximises the sum of sensitivity and specificity
coords(roc.out, "best")

# confusion matrix threshold=0.8739469
logistic.pred <- rep("Democrat", 3046)
logistic.pred[logistic.prob>0.8739469] <- "Republican"
table(logistic.pred, Party20)

CM <- table(logistic.pred, Party20)
CM <- CM[2:1, 2:1]
CM <- addmargins(CM, margin = c(1, 2))
CM

# trivial classifier 
trivial.pred <- rep("Democrat", 3046)
table(trivial.pred, Party20)

# false positive rate = FP/N
fpr <- CM["Republican", "Democrat"]/CM["Sum", "Democrat"]   
fpr 

# true negative rate = TN/N Specificity
specif <- 1 - fpr
specif

# true positive rate = TP/P Sensitivity
tpr <- CM["Republican", "Republican"]/CM["Sum", "Republican"] 
tpr 


```



## Best Subset method

Let us analyze the Best Subset method in order to compare the overall performance of the models Adjusted R2, Cp and BIC via a variable selection. The R function regsubsets() can be used to identify different best models of different sizes. We will be using a maximum of 31 variables for each model. A variable is included in a model if it is marked with the symbol '*'.


```{r}
#Best Subset Method
best_sub_df <-c(percentage20_Joe_Biden>percentage20_Donald_Trump)
regfit.full <- regsubsets(best_sub_df~., data=data[-c(1:13,19:20,27,29,31,33,46,52,53)], really.big = T,nvmax = 31)
reg.summary <- summary(regfit.full)
reg.summary$outmat
```

Another way to visualize this matrix is by using the following plots. If the space is filled by a blue square, the variable is included in the model using "x" parameters. We analize the RSS, Adjusted R2, Cp and BIC models.

```{r}
plot(regfit.full,scale="r2", col="#006699")
plot(regfit.full,scale="adjr2", col="#006699")
plot(regfit.full,scale="Cp", col="#006699")
plot(regfit.full,scale="bic", col="#006699")

data.frame(
  Adj.R2 = which.max(reg.summary$adjr2),
  CP = which.min(reg.summary$cp),
  BIC = which.min(reg.summary$bic))

```

&ensp;&ensp; 

Here, adjusted R2 tells us that the best model is the one with 29 predictor variables. However, using the BIC and Cp criteria, we should go for the model with 17 and 21 variables respectively.

Let us plot our results to visualize the min and max point we are looking for.

&ensp;&ensp;

```{r}
## RSS (residual sum of squares)
par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")

# Adjr2 with its smallest value
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted Rsq",type="l")
which.max(reg.summary$adjr2)
points(which.max(reg.summary$adjr2),reg.summary$adjr2[which.max(reg.summary$adjr2)], col="red",cex=2,pch=20)

# Mallow's Cp with its smallest value
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
which.min(reg.summary$cp)
points(which.min(reg.summary$cp),reg.summary$cp[which.min(reg.summary$cp)],col="red",cex=2,pch=20)

# BIC with its smallest value
plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
which.min(reg.summary$bic)
points(which.min(reg.summary$bic),reg.summary$bic[which.min(reg.summary$bic)],col="red",cex=2,pch=20)
par(mfrow=c(1,1))

```


&ensp;&ensp; 

## Regularization

&ensp;&ensp; 

This paragraph explains two more types of regressions, which use a regularization technique in order to avoid overfitting. It does so by imposing a penalty on the unimportant variables, which brings their coefficients towards zero. We will also be analyzing this "penalty" factor λ, which can be optimally found with simple cross validation. 

&ensp;&ensp;

## Lasso Regression

&ensp;&ensp; 

Lasso Regression (also called L1 regression) adds the “absolute value of magnitude” of the coefficient as a penalty term to the loss function. This can lead to bringing some of the coefficients to be exactly zero, i.e. some of the features are totally neglected for the evaluation of output (when the tuning parameter λ is large enough). Let us start by choosing a grid of random values of λ, we will later investigate its optimality.

```{r}
#lasso regression code
X <- model.matrix(percentage20_Joe_Biden~.,data[-c(1:13,19:20,27,29,31,33,52,53)])
y<-percentage20_Joe_Biden
grid.lasso<-10^seq(-0.5,-6,length=100)
lasso.mod<-glmnet(X,y,alpha = 1,lambda = grid.lasso)
plot(lasso.mod, xvar="lambda")



train <- sample(1:nrow(X),nrow(X)/2)
test<-(-train)
y.test<-y[test]
lasso.pred<-predict(lasso.mod,s=4,newx = X[test,],type = "response")
mean((lasso.pred-y.test)^2)

```

&ensp;&ensp; 

Let us use the cross-validation method to choose the optimal value of the parameter λ.

&ensp;&ensp; 

```{r}
# use cross-validation to choose the value of lambda 

cv.out.lasso <- cv.glmnet(X[train, ], y[train], alpha = 1,lambda = grid.lasso, nfold=10)

cv.out.lasso$lambda
summary(cv.out.lasso$lambda)

plot(cv.out.lasso)
i.bestlam.lasso <- which.min(cv.out.lasso$cvm)
i.bestlam.lasso
bestlam.lasso <- cv.out.lasso$lambda[i.bestlam.lasso]
bestlam.lasso

cv.out.lasso$cvm[i.bestlam.lasso]

lasso.pred <- predict(lasso.mod, s = bestlam.lasso,
                      newx = X[test, ])
# mean squared error

mean((lasso.pred - y.test)^2)

# residual sum of squares

lasso.RSS <- sum((lasso.pred - y.test)^2)
lasso.RSS
```

&ensp;&ensp; 

## Ridge Regression

The Ridge Regression (also called L2 regression) adds the “squared value of magnitude” of the coefficient as a penalty term to the loss function. Let us start by choosing a grid of random values of λ, we will later investigate its optimality.

&ensp;&ensp; 

```{r}
#ridge regression

X <- model.matrix(percentage20_Joe_Biden~.,data[-c(1:13,19:20,27,29,31,33,52,53)])
y<-percentage20_Joe_Biden
grid.ridge<-10^seq(2,-4,length=100)
ridge.mod<-glmnet(X,y,alpha = 0,lambda = grid.ridge)
plot(ridge.mod, xvar="lambda")



train <- sample(1:nrow(X),nrow(X)/2)
test<-(-train)
y.test<-y[test]
ridge.pred<-predict(ridge.mod,s=4,newx = X[test,],type = "response")
mean((ridge.pred-y.test)^2)

predict(ridge.mod,s=0,exact = TRUE,type = "coefficients",x=X[train,],y=y[train])[1:20,]

```

&ensp;&ensp; 

Let us use the cross-validation method to choose the optimal value of λ.

&ensp;&ensp; 
```{r}

# use cross-validation to choose the value of lambda 

cv.out <- cv.glmnet(X[train, ], y[train], alpha = 0,lambda = grid.ridge, nfold=10)

cv.out$cvm
summary(cv.out$lambda)

plot(cv.out)
i.bestlam <- which.min(cv.out$cvm)
i.bestlam 
bestlam <- cv.out$lambda[i.bestlam]
bestlam
cv.out$cvm[i.bestlam]


ridge.pred <- predict(ridge.mod, s = bestlam,
                      newx = X[test, ])
# mean squared error

mean((ridge.pred - y.test)^2)

# residual sum of squares

ridge.RSS <- sum((ridge.pred - y.test)^2)
ridge.RSS
```
&ensp;&ensp;

To determine which of the last two models has a better performance we can compare the two residual sum of squares:
&ensp;&ensp;
```{r}
lasso.RSS
ridge.RSS
```
&ensp;&ensp;
We can notice that the residual sum of square is lower for the ridge regression than for the lasso regression so we can say that the ridge regression gives a better approximation of our problem.  
From those regressions we can see that a variable that is taken into great consideration is 'Construction'. Using best subset method it appears already in the model with two variables and it has always relatively high coefficient in the regressions. To be certain of the relevance of this variable over the election result we decide to conduct a t-test over republican and democratic counties.
&ensp;&ensp; 

```{r}
t.test(Construction[c(Party20=='Republican')],Construction[c(Party20=='Democrat')])

```

This test confirmed that 'Construction' is considerably different between democratic and republican counties on average so it is in some form related with the political leaning of a county. 


### Conclusion

The USA are a very heterogeneous nation with many differences not only state by state but also internally, from county to county.
The population is not uniformly distributed and the density of population, ethnical composition and type of employment tend to have an impact on the election results.
We took New York as an example to show how the urban parts of the state are more inclusive and tend to have more progressive ideas voting the democrat party. The peripheral areas instead, tend to have more traditional views and generally prefer the republican party. 
We also considered how the ethnical differences may impact in the election results, taking the southern states as an example. We noticed that the areas with a higher percentage of Black ethnicity are more likely to avoid voting the Republicans, and tend to prefer the Democrat Party.
